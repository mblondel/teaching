{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autodiff lab work",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6lGZz2LvFrC"
      },
      "source": [
        "# Automatic differentiation lab work\n",
        "\n",
        "*Notebook prepared by Mathieu Blondel, November 2020.\n",
        "The accompanying slides are available [here](https://www.mblondel.org/teaching/autodiff-2020.pdf).*\n",
        "\n",
        "In this lab work, we are going to implement reverse differentiation (a.k.a. backpropagation) for a feedforward network (that is, the composition of a **sequence** or **chain** of functions).\n",
        "\n",
        "## Numerical differentiation utilities\n",
        "\n",
        "In this section, I define utility functions for computing Jacobians, Jacobian-vector products (VJPs), and vector Jacobian products (VJPs). You will need to use them to check the correctness of your implementations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4n6JlkxIb_V"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def num_jvp(f, x, v, eps=1e-6):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    f: a function returning an array.\n",
        "    x: an array.\n",
        "    v: an array (same shape as x).\n",
        "\n",
        "  Returns:\n",
        "    numerical_jvp\n",
        "  \"\"\"\n",
        "  if not np.array_equal(x.shape, v.shape):\n",
        "    raise ValueError(\"x and v should have the same shape.\")\n",
        "\n",
        "  return (f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n",
        "\n",
        "def num_jacobian(f, x, eps=1e-6):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    f: a function returning an array.\n",
        "    x: an array (only 1d and 2d arrays supported).\n",
        "\n",
        "  Returns:\n",
        "    numerical_jacobian\n",
        "  \"\"\"\n",
        "  def e(i):\n",
        "    ret = np.zeros_like(x)\n",
        "    ret[i] = 1\n",
        "    return ret\n",
        "\n",
        "  def E(i, j):\n",
        "    ret = np.zeros_like(x)\n",
        "    ret[i, j] = 1\n",
        "    return ret\n",
        "\n",
        "  if len(x.shape) == 1:\n",
        "    return np.array([num_jvp(f, x, e(i), eps=eps) for i in range(len(x))]).T\n",
        "  elif len(x.shape) == 2:\n",
        "    return np.array([[num_jvp(f, x, E(i, j), eps=eps) \\\n",
        "                     for i in range(x.shape[0])] \\\n",
        "                     for j in range(x.shape[1])]).T\n",
        "  else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "def num_vjp(f, x, u, eps=1e-6):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    f: a function returning an array.\n",
        "    x: an array (only 1d and 2d arrays supported).\n",
        "\n",
        "  Returns:\n",
        "    numerical_vjp\n",
        "  \"\"\"\n",
        "  J = num_jacobian(f, x, eps=eps)\n",
        "  if len(J.shape) == 2:\n",
        "    return J.T.dot(u)\n",
        "  elif len(J.shape) == 3:\n",
        "    shape = J.shape[1:]\n",
        "    J = J.reshape(J.shape[0], -1)\n",
        "    return u.dot(J).reshape(shape)\n",
        "  else:\n",
        "    raise NotImplementedError\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn8-CxAHIqVD"
      },
      "source": [
        "## Vector Jacobian products (VJPs) for basic primitives\n",
        "\n",
        "In this section, we are going to define VJPs for basic primitives. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBW-vGb8Ksra"
      },
      "source": [
        "def dot(x, W):\n",
        "  return np.dot(W, x)\n",
        "\n",
        "def dot_make_vjp(x, W):\n",
        "  def vjp(u):\n",
        "    return W.T.dot(u), np.outer(u, x)\n",
        "  return vjp\n",
        "\n",
        "dot.make_vjp = dot_make_vjp\n",
        "\n",
        "def squared_loss(y_pred, y):\n",
        "  # The code requires every output to be an array.\n",
        "  return np.array([0.5 * np.sum((y - y_pred) ** 2)])\n",
        "\n",
        "def squared_loss_make_vjp(y_pred, y):\n",
        "  diff = y_pred - y\n",
        "\n",
        "  def vjp(u):\n",
        "    return diff * u, -diff * u\n",
        "\n",
        "  return vjp\n",
        "\n",
        "squared_loss.make_vjp = squared_loss_make_vjp\n",
        "\n",
        "def add(a, b):\n",
        "  return a + b\n",
        "\n",
        "def add_make_vjp(a, b):\n",
        "  gprime = np.ones(len(a))\n",
        "\n",
        "  def vjp(u):\n",
        "    return u * gprime, u * gprime\n",
        "\n",
        "  return vjp\n",
        "\n",
        "add.make_vjp = add_make_vjp\n",
        "\n",
        "def mul(a, b):\n",
        "  return a * b\n",
        "\n",
        "def mul_make_vjp(a, b):\n",
        "  gprime_a = b\n",
        "  gprime_b = a\n",
        "\n",
        "  def vjp(u):\n",
        "    return u * gprime_a, u * gprime_b\n",
        "\n",
        "  return vjp\n",
        "\n",
        "mul.make_vjp = mul_make_vjp\n",
        "\n",
        "def exp(x):\n",
        "  return np.exp(x)\n",
        "\n",
        "def exp_make_vjp(x):\n",
        "  gprime = exp(x)\n",
        "\n",
        "  def vjp(u):\n",
        "    return u * gprime,\n",
        "\n",
        "  return vjp\n",
        "\n",
        "exp.make_vjp = exp_make_vjp\n",
        "\n",
        "def sqrt(x):\n",
        "  return np.sqrt(x)\n",
        "\n",
        "def sqrt_make_vjp(x):\n",
        "  gprime = 1. / (2 * sqrt(x))\n",
        "\n",
        "  def vjp(u):\n",
        "    return u * gprime,\n",
        "\n",
        "  return vjp\n",
        "\n",
        "sqrt.make_vjp = sqrt_make_vjp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyZTVrEQNdJR"
      },
      "source": [
        "**Exercise 1** \n",
        "\n",
        "Look at the \"exp\" and \"sqrt\"  examples above and define the primitive and its associated VJP for the relu function `relu(x) = np.maximum(x, 0)`. Check the correctness of your implementation using the `num_vjp` utility function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luk7y1NqNue2"
      },
      "source": [
        "def relu(x):\n",
        "  return \n",
        "\n",
        "rng = np.random.RandomState(0)\n",
        "x = rng.randn(5)\n",
        "u = rng.randn(5)\n",
        "\n",
        "# Check the correctness of your vjp using num_vjp:\n",
        "# num_vjp(relu.vjp, x, u)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH37U1gpN86J"
      },
      "source": [
        "## Reverse differentiation of feedforward networks\n",
        "\n",
        "Feedforward networks use a sequence of functions. The functions can either be of the form `func(x, param)` if the function has learnable parameters (e.g., `dot(x, W)`) or `func(x)` if the function doesn't have learnable parameters (e.g., `exp(x)`). \n",
        "\n",
        "We represent a feedforward network using a list of functions and a list of parameters. Let us create a small utility function for creating such a network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf-ZYYjlQb9R"
      },
      "source": [
        "def create_feed_forward(n, y, seed=0):\n",
        "  rng = np.random.RandomState(seed)\n",
        "\n",
        "  funcs = [\n",
        "    dot,\n",
        "    relu,\n",
        "    dot,\n",
        "    relu,\n",
        "    dot,\n",
        "    squared_loss\n",
        "  ]\n",
        "\n",
        "  params = [\n",
        "    rng.randn(3, n),\n",
        "    None,\n",
        "    rng.randn(4, 3),\n",
        "    None,\n",
        "    rng.randn(1, 4),\n",
        "    y\n",
        "  ]\n",
        "\n",
        "  return funcs, params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TEs4xEGQySA"
      },
      "source": [
        "Next, let us create a small utility function for correctly calling each function, depending on whether it has 1 or 2 arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8-rUfSlQ6-s"
      },
      "source": [
        "def call_func(x, func, param):\n",
        "  \"\"\"Make sure the function is called with the correct number of arguments.\"\"\"\n",
        "\n",
        "  if param is None:\n",
        "    # Unary function\n",
        "    return func(x)\n",
        "  else:\n",
        "    # Binary function\n",
        "    return func(x, param)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcR14zoMRPUC"
      },
      "source": [
        "**Exercise 2.** \n",
        "\n",
        "Implement the following function for evaluating the feedforward network. Check that the returned value is correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmnpMz9bRx-Q"
      },
      "source": [
        "def evaluate_chain(x, funcs, params, return_all=False):\n",
        "  \"\"\"\n",
        "  Evaluate a chain of functions.\n",
        "\n",
        "  Args:\n",
        "    x: initial input to the chain.\n",
        "    funcs: a list of functions of the form func(x) or func(x, param).\n",
        "    params: a list of parameters, with len(params) = len(funcs).\n",
        "            If a function doesn't have parameters, use None.\n",
        "    return_all: whether to return all intermediate values or only the last one.\n",
        "\n",
        "  Returns:\n",
        "    value (return_all == False) or values (return_all=True)\n",
        "  \"\"\"\n",
        "  if len(funcs) != len(params):\n",
        "    raise ValueError(\"len(funcs) and len(params) should be equal.\")\n",
        "\n",
        "  if return_all:\n",
        "    return\n",
        "  else:\n",
        "    return\n",
        "\n",
        "rng = np.random.RandomState(0)\n",
        "x = rng.randn(2)\n",
        "y = 1.5\n",
        "\n",
        "funcs, params = create_feed_forward(n=len(x), y=y, seed=0)\n",
        "W1, _, W3, _, W5, y = params\n",
        "\n",
        "# Make sure that `evaluate_chain(x, funcs, params)` returns the same value as\n",
        "# a manual implementaton:\n",
        "# x1 = dot(x, W1)\n",
        "# x2 = relu(x1)\n",
        "# x3 = dot(x2, W3)\n",
        "# x4 = relu(x3)\n",
        "# x5 = dot(x4, W5)\n",
        "# value = squared_loss(x5, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuZNM-6yT0gp"
      },
      "source": [
        "**Exercise 3.**\n",
        "\n",
        "Reusing the previous function with `return_all=True`, implement the following function that returns both the network value and the Jacobian w.r.t. `x`. Check correctness of the Jacobian using `num_jacobian`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmXeQ34TULOP"
      },
      "source": [
        "def call_vjp(x, func, param, u):\n",
        "  \"\"\"Make sure the vjp is called with the correct number of arguments.\"\"\"\n",
        "  if param is None:\n",
        "    vjp = func.make_vjp(x)\n",
        "    vjp_x, = vjp(u)\n",
        "    vjp_param = None\n",
        "  else:\n",
        "    vjp = func.make_vjp(x, param)\n",
        "    vjp_x, vjp_param = vjp(u)\n",
        "  return vjp_x, vjp_param\n",
        "\n",
        "\n",
        "def reverse_diff_chain(x, funcs, params):\n",
        "  \"\"\"\n",
        "  Reverse-mode differentiation of a chain of computations.\n",
        "\n",
        "  Args:\n",
        "    x: initial input to the chain.\n",
        "    funcs: a list of functions of the form func(x) or func(x, param).\n",
        "    params: a list of parameters, with len(params) = len(funcs).\n",
        "            If a function doesn't have parameters, use None.\n",
        "\n",
        "  Returns:\n",
        "    value, Jacobian w.r.t. x\n",
        "  \"\"\"\n",
        "  # Evaluate the feedforward model and store intermediate computations,\n",
        "  # as they will be needed during the backward pass.\n",
        "  xs = evaluate_chain(x, funcs, params, return_all=True)\n",
        "\n",
        "  m = xs[-1].shape[0]  # Output size\n",
        "  K = len(funcs)  # Number of functions.\n",
        "\n",
        "  # We need a list as the shape of U can change.\n",
        "  U = list(np.eye(m))\n",
        "\n",
        "  for k in reversed(range(K)):\n",
        "    # Implement backward differentiation here\n",
        "\n",
        "  return xs[-1], np.array(U)\n",
        "\n",
        "# Check correctness of Jacobian using `num_jacobian`.\n",
        "# def f(x):\n",
        "#   return evaluate_chain(x, funcs, params)\n",
        "# # num_jacobian only accepts functions of one argument.\n",
        "# num_jac = num_jacobian(f, x)\n",
        "# value, jac = reverse_diff_chain(x, funcs, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fvXX-HaVz4Y"
      },
      "source": [
        "**Bonus exercise.**\n",
        "\n",
        "Modify the above function to also return the Jacobians w.r.t. W1, W3, W5. Check correctness using `num_jacobian`."
      ]
    }
  ]
}